\documentclass[10pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{sectsty}
\usepackage{caption}
\usepackage{soul}
\usepackage{color}
%\usepackage{hyperref}
\usepackage[draft=false,letterpaper,breaklinks,colorlinks,linktocpage,citecolor=blue,linkcolor=blue,urlcolor=blue]{hyperref}
\usepackage[compact]{titlesec}
\usepackage[super]{nth}
\usepackage[title]{appendix}
\usepackage[backend=biber,style=numeric,citestyle=numeric,sorting=ynt]{biblatex}

\bibliography{libri-whitepaper}

\allsectionsfont{\sffamily}
\renewcommand{\baselinestretch}{1.1}
\setlength{\parskip}{1em}
\setlist[itemize]{topsep=0pt,itemsep=0pt,partopsep=1ex,parsep=1ex}
\setlist[enumerate]{topsep=0pt,itemsep=0pt,partopsep=1ex,parsep=1ex}

\newcommand{\ttt}[1]{\texttt{#1}}
\def\Entry{\ttt{Entry}}
\def\Page{\ttt{Page}}
\def\Envelope{\ttt{Envelope}}
\def\Publication{\ttt{Publication}}
\def\Put{\ttt{Put}}
\def\Get{\ttt{Get}}


\begin{document}

\title{Libri: opinionated, high-performance decentralized storage for health data}
\author{Drausin Wulsin}
\maketitle

\begin{abstract}
TODO
\end{abstract}

\section{Introduction}
% - problems w/ health data
% 	- existing orgs
% 	- why so hard to share
% 	- security
% - Libri solution
% 	- neutral, durable storage for any health data
% 	- problems it does not try to solve (right now)
% - other medical-data related projects

The US healthcare system is very decentralized, with no single provider of healthcare and thus no single place where everyone's health data lives. The payer/insurance side of the industry is also decentralized, with many different public and private entities, large and small. This decentralizations creates many problems for patients. Libri is an effort to solve one of these problems: patients have almost no visibility or control over their own health data.

Many people have experience this problem first hand when they need to switch doctors because of a move or change in insurance or when they need to coordinate among a number of different healthcare providers, as in an orthopedic proceedure like a knee replacement. Unfortunately, this process still involves numerous phone calls, faxes, and often hard-copy paper or CDs. Despite the existance of efficient, secure forms of data transfer like SFTP and encrypted email for over 20 years, these arcane, analog data transfer methods still comprise the majority of how health data is managed today. 

We believe that the solution to this problem is for all organizations in the healthcare ecosystem to read and write data to the same place. That place should not be controlled by a single entity (corporate or otherwise), should run on open source software, should enforce strong encryption and security, and should be both performant and durable. Libri satisfies these criteria. While Libri was designed from the start with the health data storage and sharing use case in mind, nothing about its API or internal workings is specific to healthcare. This separation is intentional; it allows us to focus solely on the data storage problem and avoid distraction and complications from the many other important and hard problems in healthcare technology. This focus also allows us to avoid additional complexity added by a blockchains, smart contracts, and decentralized identity management.

We describe Libri in detail in the rest of this paper. Section \ref{sec:related} covers a high-level overview of related decentralized storage technologies, the current state of health data sharing technology, and mentiones some other decentralized healthcare network efforts. Section \ref{sec:arch} introduces the high-level Libri responsibilities and architecture. Section \ref{sec:impl} describes some implementation details around how we envision organizations deploying and managing their fleet of Libri nodes. Section \ref{sec:exps} discusses some initial experiments and promising performance results. Finally, Section \ref{sec:future} gives our high-level vision for the Libri community and what technical work we see before us.

\section{Related work}
\label{sec:related}

\subsection{Decentralized storage}
% - decentralized storage landscape
% 	- non-blockchain (Kademlia, BitTorrent, IPFS)
% 	- blockchain (Sia, Storj, BigchainDB, Ethereum Swarm, Filecoin)
% - challenges of blockchain
% 	- scale
% 	- maintainability

Decentralized storage has existed for decades, but BitTorrent [CITE], released in the early 2000s, was one of the first very successful networks. BitTorrent makes peer-to-peer file storage efficient and contains incentives (between "seeders" and "leechers") in order to keep the network healthy. Kademlia \cite{Kademlia} is the distributed hash table (DHT) used by almost all decentralized storage system these days (including BitTorrent, which uses it for decentralized peer tracking). It provides a simple, efficient protocol for peers to use when finding and storing values among each other. 
% TODO
% 	- S/Kadelmia
% 	- Coral
IPFS [CITE] uses much of the same design as BitTorrent---peer-to-peer file sharing, possible caching of popular documents, tit-for-tat peer-to-peer incentive accounting---but it addresses the data stored by a hash of its content instead of a filename. This content-addressing combined with a simple link structure allows it to behave as a Merkle DAG, giving it great flexibility in being able to store and model many forms of data, from simple blobs to files to whole filesystems. At its heart, though, IPFS looks very much like BitTorrent: peers host data that others may optionally copy and host as well, and the addresses of the peers hosting each object are stored in a Kademlia DHT.

Peer-to-peer data storage and sharing networks are powerful, but the networks themselves are unable to offer any guarantees about the data stored in them. If the only peer hosting some data goes offline, that data is lost to the network. Over the last few years, blockchain-based networks like Sia [CITE], Storj [CITE], BigchainDB [CITE], Ethereum Swarm [CITE], and Filecoin [CITE] have been developed in an attempt to solve the incentives problem: why should any peer dedicate storage space, bandwidth, and machine resources? These different networks each tackle this problem in slightly different ways, but they all rely on an economic incentive for peers to store (and keep storing) data. This economic incentive requires transactions betwen parties, and those transactions require a blockchain. 

We expect most readers to be familiar with at least the high-level concepts of a blockchain. Transactions are gossiped among a network of peers, which work (together but also in competition with each other) to order and package blocks of those transactions together. Transactions and blocks are immuntable. Each block points to a previous block, forming a chain. So, if I have some data that I want to store in a decentralized network, instead of having to host it myself, I can pay another peer (or peers) to do so instead. These decentralized storage economies are still in their early periods with major technical, stability, and economic details still being worked out. 

While the incentives that a blockchain allows adds an important feature to a storage network, it also saddles a network with a slow, inefficient, and complex component. [TODO: more discussion re blockchain issues].
% TODO
% - need to get some blockchain scaling references
% 	- maybe read about Ripple and/or Stellar?
% 	- does Vitalik B partitioning scheme exist yet?
% - transaction throughput and blocktime performance
% - code/network complexity and management


\subsection{Health data sharing}
% - patient portals
% - centralized
% 	- HIEs
% 	- Picnic Health
% 	- Apple Health
% - decentralized
% 	- MedRec, DokChain, MedicalChain, MediBloc, Coral Health, Patientory, (others?)
Today, if patients have access to their electronic medical records (EMRs), it is most often through a health system's patient portal, where patients have authenticated access into a site showing things like lab results, appointments, and prescriptions. While these portals are indeed better than nothing, they rarely contain the more detailed EMR documents (e.g., clinical notes), and they often omit EMRs from whole groups within the system that aren't yet integrated with it. Many small-to-medium sized practices don't have any patient portal. These patient portals are also read-only: patients cannot upload their EMRs from portal A into portal B. Some portals are part of regional health information exchanges (HIEs), where some doctors can log in and see records from others, but like the portals, these HIEs have incomplete EMR and doctor coverage even within a large metropolitan regions. Some health systems are exposing RESTful APIs (often in the form of the HL7 FHIR standard) that allow \nth{3} parties like Apple Health to query and aggregate them. While these APIs are good progress, the vast majority of doctors and EMRs are not accessible via them. Like the patient portals (which may in fact use them), these APIs are also generally read only for clinical data and thus do not fully solve the problem of transfering a subset of EMRs from doctor A to doctor B.

In recent years, a bevy of blockchain-based EMR storage proposals have appeared, including DokChain, MedRec, MedicalChain, MediBloc, Coral Health, Patientory [CITES NEEDED]. [FEW SENTANCES SUMMARIZING EACH AND GIVING STATUS.] While we are glad to see similar ideas taking shape across the healthcare ecosystem, most of these efforts are still very much in their infancy (and some are mere proposals), often with very little technical details available to the public. 

\subsection{Libri's place}
Libri focuses only on storing and sharing data in a decentralized manner. We believe this decentralization is necessary to include as many organizationsn in the healthcare ecosytem as possible. Higher-level needs like authentication, identity, and EMR-integrations will be built on top of Libri but most likely as conventional, centralized applications talking to the shared Libri network. Healthcare business logic is hard enough to manage when building in a centralized world; we don't see the need to complicate higher-level logic as well by prematurely decentralizing it. A decentralized storage core is sufficient.


\section{Architecture}
\label{sec:arch}
% - librarian & author nomenclature
% - librarian responsibilitiee
% 	- storing & returning values for given key
% 	- keeping track of (some of) other peers in the network
% 	- gossiping publication events
% 	- re-replicating data if it becomes under-replicated
% - author responsibilities
% 	- uploading, downloading, & sharing documents to/from Libri
% 		- compression
% 		- pagination
% 		- end-to-end encryption
% - no blockchain
% 	- instead of single, immutable log of all storage events, each node has its own log
% 	- if you want history, from before your node, consult published logs of other nodes

Peers in the Libri network are called Librarians, and clients of these peers are called Authors. Librarians are responsible for 
\begin{itemize}
	\item storing and retrieving documents,
	\item maintaing routing table of other peers in the network,
	\item gossiping store events to other peers,
	\item re-replicating data if it becomes under-replicated.
\end{itemize}
The storing/retrieving and routing table behave quite similarly to a standard Kademlia \cite{kademlia} storage network, and the store event gossiping and re-replication responsibilities add two important capabilities on top of it. Section \ref{sec:API} below describes the Librarian API in more detail.

Each Librarian and Author has an identity defined by the 256-bit public key of an secp256k1 ECDSA key-pair. Librarians and Authors sign each of their requests with their private key (see Section \ref{sec:Identity} below for more details), and each Libarian's public key also defines its location on the Kademlia hash ring. Using a public key for peer identity and request signing closely follows the approach in S/Kademlia, though the node ID is just the public key rather than a hash of it as in S/Kademlia. 

Authors are clients responsible for uploading, downloading, and sharing content in the Libri network. In these capacities, they handle
\begin{itemize}
	\item compression \& decompression,
	\item pagination \& concatenation,
	\item end-to-end encryption \& decryption,
	\item Libri network Put \& Get requests.
\end{itemize}

Author clients by default compress all data with gzip unless otherwise specified. This compressed stream is then split up into pages (or chunks) of 2MB or less. Each page is individually encrypted and uploaded (via a Put request) to Libri. When retrieving data from Libri, the author client is also responsible for the reverse process of downloading, decrypting, concatenating, and decompressing the chunks into the orignial byte stream that was uploaded.

Both Librarians and Authors may subscribe to the gossipped store events of other Librarians. Usually each Librarian or Authors will want to receive a complete log of all events. In a blockchain-based system, there might be a single log. In Libri, each client maintains its own log, which with very high probability captures all store event. The individual logs of each client should have exactly the same events in them, but very small differences in ordering and timing (< 1s) will exist due to gossip path differences. We believe these small differences are well worth the performance benefits and implementation simplicity of avoiding a blockchain. We expect organizations running fleets of nodes will post consolidated versions of these store logs for the public to download and compare, if they desire.

\subsection{Librarian API}
\label{sec:API}
% - storage: same Kademlia framework as others
% - API
%	- keys hashes of serialized docs
% 	- documents
% 		- immutability
% 		- entries, pages, & envelopes
% 		- E2E enc baked into API
% - librarians peers
% 	- mechanics of a simple Put & Get
Each Librarian exposes a synchronous service with the following endpoints:
\begin{itemize}
	\item \ttt{Introduce} receives a peer ID and IP address and returns a random sample of other peers from the routing table.
	\item \ttt{Find} receives a key and returns either the value for that key if present or the closest peers from the routing table to it.
	\item \ttt{Verify} receives a key and an HMAC key and returns either the HMAC-SHA-256 of the value for that key, if present, or the closest peers from the routing table to it.
	\item \ttt{Store} receives a key-value pair to store at that peer.
	\item \ttt{Get} receives a key and returns the value for that key, if it exists, managing the recursive \texttt{Find} operations for the client.
	\item \ttt{Put} receives a key-value pair and stores the value in the appropriate peers, managing the recursive \texttt{Find} and final Store operations for the client.
	\item \ttt{Subscribe} receives reader and author public key Bloom filters and (continually) streams Publications matching the filters to the client.
\end{itemize}

The \ttt{Find} and \texttt{Store} endpoints follow the standard Kademlia protocol. \texttt{Introduce} is used when a new peer is created and needs to bootstrap some initial other peers into its routing table. \texttt{Verify}, used by peers to ensure sufficient replication among other peers of the documents they contain, behaves very simiarlly to \texttt{Find} except that it returns an HMAC instead of the value. \Get{} and \Put{} are mostly intended for Authors to call (rather than other Librarians), especially since they involve the receiver of the call making a number of \texttt{Find} and/or \texttt{Store} calls. In Section \ref{sec:Incentives}, we discuss authorization and rate limits for endpoints between peers and clients.

\ttt{Subscribe} allows peers to listen to the publications (eminating from or relayed through) other peers. Librarians will usually subscribe to the $O(10)$ other Librarians, receiving almost all of each subscribed peer's publications, meaning that each peer would receive at least one publication notification for every true publication event with very high probability. Authors not interested in the full publication log would instead subscribe to $O(10)$ Librarians with Bloom filters for specific sets of author and/or reader public keys that it is interested in.

\subsection{Libri Documents}
\label{sec:Docs}
% - key & binary value
% - doc types
% 	- entry
% 	- page
% 	- envelope
% - publications
Libri documents take one of three forms:
\begin{itemize}
	\item A \Page{} holds a particular sequential chunk of the (compressed) document content. 
	\item An \Entry{} holds the encrypted contents of the document, either as a single Page, or as a list of keys to separate Page documents.
	\item An \Envelope{} contains the encrypted entry encryption key (EEK) between a given author and a given reader.
\end{itemize}
Documents are serialized to a binary representation for storage in Libri. The key of any document is the SHA-256 hash of its value bytes. Below we discuss some particulares of each document type.

\subsubsection{Page}
A \Page{} contains some or all of the content for a single document. The maximum \Page {} content size is 2 MB, from our assumption that at least 50\% of documents will have sizes less than 2 MB. For now, our pagination strategy is very simple: we just split the (compressed) plaintext of the content into consecutive 2 MB chunks. We believe that this staightforward approach, combined with a reasonable replication strategy (e.g., 3-5x) will be sufficient to ensure clients always have access to the requisite chunks required to reconstruct their document. If this assumption proves problematic, we can always implement more sophisticated chunking via erasure coding in the clients. The Libri server code is agnostic to the chunking strategy.

Each \Page{} document has the public key of the Author that created it, the index of the particular \Page{} to define the order in which subsequent \Page{}'s plaintext content should be concatenated, the ciphertext of the \Page{}'s portion of the content, and the ciphertext MAC. 

\subsubsection{Entry}
An \Entry{} defines the content of the document. If the total content can fit on a single \Page{}, the \Entry{} contains that \Page{} within it. That single \Page{} is not stored separately in Libri. If the content is large enough to require splitting across multiple \Page{}s, the \Entry{} just contains the keys (i.e,. hashes of the binary \Page{} document) of the relevant \Page{} documents. Entries also contain the public key of the author, a creation timestamp, and encrypted metadata and a corresponding MAC. 

Entry metadata contains attributes like the media/MIME type of the data, compression codec, full byte size of the entire document (across all \Page{}s), and content schema references. See Appendix \ref{app:entry-meta} for more details. 

A random 108-byte entry encryption key (EEK) is generated whenever an \Entry{} is created. The EEK contains four sub-keys:
\begin{itemize}
	\item 32-byte AES-256 key for \Page{} and entry metadata encryption,
	\item 32-byte \Page{} block cipher initialization vector (IV) seed,
	\item 32-byte HMAC-SHA-256 key for ciphertext MACs,
	\item 12-byte metadata block cipher IV.
\end{itemize}
The contents in each \Page{} and the \Entry{} metadata are encrypted via an AES-256 GCM block cipher. \Page{} $i$'s plaintext content is encrypted using the EEK AES-256 key and a per-page IV generated from the first 12 bytes of \ttt{HMAC-SHA-256(IV seed, i)}, and a MAC for this ciphertext is calculated from the EEK HMAC key via \ttt{HMAC-SHA-256(HMAC Key, Page Ciphertext)}. The \Entry{} metadata is serialized to its Protobuf binary representation and encrypted with the EEK AES-256 key and the 12-byte EEK metadata IV. 

\subsubsection{Envelope}
Envelopes exist solely for the purpose of sharing EEKs from the author of a document to a reader (identified here just by one of its public key). The 108 byte EEK is encrypted in the \Envelope{} using the key encryption key (KEK) derived from the shared ECDH secret between the author and reader keys. The KEK contains three sub-keys:
\begin{itemize}
	\item 32-byte AES-256 key for EEK encryption,
	\item 12-byte block cipher IV,
	\item 32-byte HMAC-SHA-256 key for ciphertext MAC.
\end{itemize}
This 76-byte KEK is derived from a SHA-256 hash-based key derivation function (HKDF) initialized with the x-coordinate of the shared ECDH secret. The 108 bytes of the EEK are encrypted via an AES-256 GCM block cipher using the AES-256 key and block cipher IV in the KEK. A MAC of the resulting ciphertext is also calculated via \ttt{HMAC-SHA-256(HMAC Key, IV)}.

A complete \Envelope{} contains
\begin{itemize}
	\item document key of entry whose EEK this envelope encrypts,
	\item author public key,
	\item reader public key,
	\item EEK ciphertext,
	\item EEK ciphertext MAC.
\end{itemize}
Since usually an author wants to be able to later read a document they create, they usually first send an \Envelope{} to themselves, using another one of their public keys as the reader public key. This self-share allows clients to avoid storing the EEK plaintext, since they can always decrypt their self-shared \Envelope{} if/when they want to share the \Entry{} with someone else.

When they do want to share a document with someone else, they create a new \Envelope{} re-encrypting the EEK using the KEK derived from the shared ECDH secret between one of their key-pairs and one of the reader. A reader must then only monitor the \Envelope{} publications for those with author or reader public keys that they know they own in order to see what \Entry{}s they can decrypt.

\subsubsection{Publication}

\Envelope{} storage events are gossiped between Librarian peers in the form of \Publication{} messages, which are exchanged via the streaming \ttt{Subscribe} request from one peer to another. A \Publication{} contains a subset of the fields of the \Envelope{}:
\begin{itemize}
	\item envelope document key
	\item entry document key
	\item author public key
	\item reader public key
\end{itemize}

Each Librarian has its own \Publication{} stream, which it constructs from \ttt{Subscribe} requests to other Librarians and sends to Librarians that are subscribed to it. Because these \texttt{Publication}s are gossiped, one Librarian may have a slightly different \Publication{} order than another, but the differences should be quite small ($<$ 1s). Organizations running Librarian peers may wish to set up Author clients \ttt{Subscribe}d to them so that the clients can save these \Publication{}s to some more durable storage like a database or message queue. 

\subsection{Identity}
\label{sec:Identity}
% - identity
% 	- author & reader keys
% 	- organizations
% 	- known orgs & trust
% 	- request signatures
Like most other cryptographic systems, Author and Librarian identity relies on elliptic curve (secp256k1) public keys. Each request to the Libri API contains metadata with a unique, random 32-byte request ID and the public key of the requester. Requesters create a JSON web token (JWT) containing a single claim---the SHA-256 hash of the Protobuf binary message---and sign it with their private key. 

Organizations will typically run a fleet of Librarians, perhaps 8 or 16. Each Librarian will have its own distinct ID, but the organization may wish to identify all of them as belonging to the same organization. When this is the case, the organization generates its own public-private key pair and securely distributes that key pair to each of its Librarians. Each Librarian then includes the oranization public key in the request metadata and also signs the same JWT with its organization public key.

When Librarians receive requests, they first verify the peer public key signature of that request. If an organization signature is present, they verify that as well. Organization IDs in particular allow a Librarian to segment the requests it receives into tiers of trust. Organization A could configure its Librarians to trust requests from organizations B, C, \& D, whereas other requests may be treated more skeptically. We discuss the authorization and rate limit results of these differentiated trust tiers below in Section \ref{sec:Incentives}.

\subsection{Authors}
\label{sec:Authors}
% - author clients
% 	- compression
% 	- pagination
% 	- end-to-end encryption
% 	- mechanics of simple doc upload & share

Authors are the clients of the Libri network: they write and read documents. To convert binary content into the documents to be written to the network, authors follow a basic process of compression, pagination, and encryption. 

Compression is optional but recommended, since its performance cost is usually low and can result in much smaller files that need to be uploaded and stored. The compression codec used is included in the \Entry{} metadata. The compressed content is then split up---paginated---into chunks of at most 2 MB. Each of these chunks is then individually encrypted using the EEK, which is generated randomly for each content. The EEK is encrypted by the KEK generated from the shared author-reader ECDH secret and is used to create the \Envelope{}. When small content requires only a single \Page{}, that \Page{} is stored within the \Entry{} for the content. The Author then \ttt{Put}s the \Envelope{} and \Entry{} into the network via calls to one or more Librarians, which store each document with the appropriate other Librarians close to its key in the Kademlia hash ring. 

Downloading a document follows a similar pattern in reverse. The Author client first \ttt{Get}s the \Envelope{} for a given document key and confirms that it has the private key indicated by the reader public key in the \Envelope{}. Assuming it does, it constructs the KEK and uses it to decrypt the EEK ciphertext. It the \texttt{Get}s the \Entry{} indicated by the \Entry{} key in the \Envelope{} followed possibly by the additional \Page{}s indicated in the \Entry{} and uses the EEK to decrypt the \Entry{} metadata, which gives---among other things---the compression codec used. \Page{}s are iteratively decrypted, concatenated, and decompressed to for the final binary content. 

When one Author wants to share a document with another (which we designate the Reader), they only need to create a new \Envelope{}, since the Reader just needs to receive the (encrypted) EEK rather than the whole document re-encrypted. The Author gets one of the Reader's public keys \footnote{The Author gets one of the Reader's public keys either directly via email or QR code or indirectly via a 3rd party that the Reader has registered some of its public keys with} and samples one of its own key-pairs to construct the KEK. The Author then encryptes the EEK with the new KEK and includes its public key, the Reader's public key, the key of the \Entry{} it is sharing, and the EEK ciphertext in the newly constructed \Envelope{}. 

Each Author client maintains two sets of key-pairs: author keys and reader keys. Technically, each Author client could just have a single key-pair they use to send and receive documents. But if every entity using Libri only had a single key-pair identifying them, one could potentially re-identify patients based on their (data-sharing) relationships with doctors. Similar re-identification attacks have been shown to be successful on "anonymized" credit card transactions [CITE]. Since even the knowledge that a patient visits a particular doctor is protected health information (PHI), Author clients must have more than one key-pair. For a further layer of anonymity, the keys clients use to send documents (the author keys) are distinct from those they use to receive them (the reader keys). This distinction means that knowing one of they keys a doctor uses to receive documents a patient might share with them does not let the patient see even a subset of the documents that doctor is sharing with other patients. In our initial Author client implementation, each Author has 64 author key-pairs and 64 reader key-pairs. Each key-pair is individually encrypted via scrypt [CITE] using a master password.

\subsection{Incentives \& Authorization}
\label{sec:Incentives}
% - incentives
% 	- large orgs: access to data
% 		- less incentive for smaller orgs and indivs: ok!
% 	- rate limits
% 	- authorization
% 	- health checks
% - authorization

A key result in Libri's avoidance of a blockchain and tokens is the lack of any economic incentives for peers to participate in the network. This absence of economic incentives in the face of real economic costs \footnote{We estimate that a modest 8-peer fleet at 1 CPU, 6 GB RAM, \& 100 GB storage per peer would cost about \$400 per month on Google Cloud Platform.} requires that the organizations get some benefit to offset the cost. We believe this benefit is unmediated, read/write access to what we hope will become a massive repository of encrypted healthcare data. While ~\$5000 per year of infrastructure costs certainly is not nothing, is it small relative to the currently alternatives: paying humans to mediate this data exchange through more analog methods (fax, snail mail, CDs in padded envelopes, or even wrangling email attachments). These economics do not make much sense for individual hobbiests in the way they do for other decentralized systems, but they do for larger organizations that have much to gain from simple, efficient access to what will be a vast repository of health data. We hope that government organizations will participate as well, especially since a durable repository of all health data should be considered a public good and thus worth supporting for the benefit of all. Since hobbiests will have much less incentive to run peers, we expect that the peers run by organizations will have much higher uptime (basically 100\%) and more consistent resource guarantees, which together ensure that the network as a whole operates efficiently and with lower performance variance than other decentralized systems.

Since requests between Libri peers are "free" of economic costs, we restrict their usage via simple rate limiting. Each peer maintaings per-second and per-day rate limit counters for requests from every other peer and for each Libri endpoint. They also maintain limits on the number of distinct peers they may receive requests from per second and per day. Peers have the concept of "knowing" other peers, or the organizations they belong to. If a peer is configured (e.g., via a peer and/or organization ID whitelist) to know certain other peers, it may allow higher rate limits from those peer. The extreme example of this form of authorization is that an organization probably will want to restrict \ttt{Get} and \texttt{Put} endpoint requests to only its own clients, since each \texttt{Put} and \texttt{Get} request requires the server to make a number of additional \texttt{Find} and \texttt{Store} requests. 

Occasionally, Librarians peers will fall into a bad or unavailable state, perhaps because they are momentarily undergoing some sort of routine maintenace (e.g., being redeployed with an updated version or having their local storage backed up) or because they are legitimately no longer available. In either case, we wish to proacively avoid sending requests to those peers. Each Librarian maintains a current healthy vs. not-healthy state of every other Librarian in its routing table. This state is updated after every request (both successful and unsuccessful) to another Librarian. Each Librarian also has a healthcheck endpoint that others may call to confirm that it is up and receiving requests. By avoiding sending requets to known-unhealthy peers, Librarians reduce the variance and latecy of some of the more involved query patterns, like those required for \ttt{Get}s and \texttt{Put}s.

\subsection{Durability}
% - durability
% 	- replication & verification
% 	- peer & org drop outs

When clients store documents in Libri, they store them in the network as a whole, rather than with specific peers, as is the case with other decentralized storage networks. The Libri network is thus responsible for ensuring that, onece stored, documents are never lost due to peer drop-outs or network connectivity issues. Librarian peers thus are responsible for maintaining their own part of the hash ring, ensuring that documents they're storing are sufficiently replicated and storing additional copies in other peers if they become under-replicated. 

Let's say that organizations A, B, C, \& D are each running a fleet of 8 peers, but then organization D decides to stop running these peers. A, B, \& C are responsible for storing the additional copies of documents. These peers must first detect that documents are under-replicated and then send additional \ttt{Store} requests to other peers to bring the replication level back to normal. 

In addition to server synchronous requests, each Librarian also has an asynchronous process that loops through the documents in its own internal storage. For each document, it sends out \ttt{Verify} requests, which behaves almost exactly like the \texttt{Find} requests except that if the peer has the document, instead of returning the value, it returns a \texttt{HMAC-SHA-256} of the value using an HMAC key given in the request, thereby proving that they do in fact have the document of interest. Most verification operations will conclude with the other peers storing the replicas successfully proving they they do indeed have the value stored. 

If one of the peers that once stored the value no longer has it or is unavailable, the verifier will detect at the document is underreplicated and will issue a series of \ttt{Store} requests (the same as what would occur during a \texttt{Put}) in order to re-store the document on additional peers. Because each peer stores documents with keys close to its ID on the hash ring, the requests it will make to verify and possibly re-replicate documents will be within its local neighborhood on the ring and thus will be fast and efficient.

Librarians currently wait 1 second between verifications, meaning that if a Librarian is storing $n$ documents, it is verifying the replication of each document on a period of $n$ seconds. Assuming a replication factor of $k$, the network as a whole is thus verifiying each document on average $k/n$ seconds. It is hard to know exactly what the "right" verification period should be. We expect to monitor how quickly the network is able to "heal" itself after losing peers and update the replication period accordingly.

\subsection{Protecting against malicious actors}
% - protecting against malicious activity
% 	- DDOS, spam, Sybil

As with any decentralized system where one does not necessarily have identity and/or reputation information for every peer, Libri is designed to be resiliant to many different types of malicious actors. Below we discuss some of the most common forms of attack and Libri's defense against them. 

\subsubsection{DDOS}
While decentralized systems are intended to be more resiliant to distributed denial of service (DDOS) attacks than traditional, centralized services, many---Libri included---are probably small enough that a well-equipped group could reasonably DDOS all nodes. Each organization running Librarian peers could choose to change their firewall rules to block all external traffic, partitioning their fleet from the rest of the (overwhelmed) network, and operate in degraded read-only mode. Depending on the overall size of the nextwork and the number of peers in their fleet, organizations might still have access to a non-trivial subset of the documents.

We also expect that organizations running nodes to usually maintain some local copy of the subset of data in Libri they care about, since interacting with that local copy will always be faster and less variable than the Libri network. In the face of a prolonged, Libri-wide DDOS attack, organizations may decide to fall back to degraded read-only mode against their local caches and pause writes to Libri until the attack has subsided.

Of course, adding nodes and partner organizations to increase the size and diversity of the network is the best defense against DDOS.

\subsubsection{Spam}
A spam attack might take the form of one or more clients issueing many \Get{} or \Put{} requests, potentially trying to download or upload many terrabytes of data in order to overwhelm the network. We expect that unknown vs. known peer and organization rate limits will reduce what otherwise could be a massive flood of requests to a much smaller fraction of the total. For example, a Libraian might only accept 1 \ttt{Find} request per second from all unknown Librarians but 50 requests per second from all of its known peers and organizations. It might accept zero \ttt{Store} requets from unknown peers and 25 requests per second from its known peers. While such rate limiting doesn't eliminate the spam problem entirely, it can reduce its impact to be just a small fraction of the overall requests.

\subsubsection{Honest Gepeto}
In the honest Gepeto (CITE Storj) attack, an organization might run legitmate, well-behaved nodes for some time before pulling their peers off the network all at once. Since each document is replicated a number of times, the probability that their nodes would contain all copies of some document is quite low, effectively eliminating the risk of full data loss. But the documents these peers did store would now be under-replicated. The remaining peers in the neighborhood of each pulled peer would manage re-replicating documents up to their sufficient replication level.

\subsubsection{Sybil}
A Sybil attack occurs when one actor creates many peers in order to disrupt the network's regular operation, perhaps by ignoring or disobeying requests (e.g., not storing data wehen it says that it has). Like DDOS, the feasibility of this attack is inversely proportional to the size of the network. Furthermore, since peers maintaining whitelists of "known" organizations will likely have much strictly rate limits on unknown peers, new peers may not receive many requests or have the opportunity to send many bad requests.



\section{Implementation}
\label{sec:impl}

Libri is implemented to be as simple to develop and maintain as possible. We thus use off-the-shelf tools when available and strive to be specific and opinionated rather than overly flexible and generic. Below we describe some implementation details. 

\subsection{Librarian peers}
Librarian peers are intended to be run from Docker containers in a cloud provider, like Google or Amazon. These containers are orchestrated via Terraform and Kubernetes, which also manage the persistent SSDs attached to each container for storage, a Prometheus server for monitoring and alerting, and Grafana server for dashboards. While it certainly is possible to run a Librarian peer on a laptop, we orient towards cloud deployment and infrastructure to standardize configuration and make use of the superior reliability, performance, and features offered there. 

Each Librarian exposes an RPC service over http. The service interface is defined in GRPC, which uses Protobuf for message serialization. GRPC has been battle-tested at Google for over the last decade and has server and client libraries in most common languages. It also has nice features like streaming endpoints, which we use when gossiping publication events between peers. We expect to only develop and maintain a single server implementation in Golang. 

Librarians use RocksDB for local storage. RocksDB is an embedded key-value store maintained by Facebook and is optimized for fast writes on SSDs. Each Librarian's RocksDB directory is written to a network-attached SSD volume, which is incrementally backed up to durable cloud storage (e.g., S3). 


\subsection{Hypothetical organization's setup}

An organization runs peers to get read/write access to the DHT as well as the stream of all publication events. A modest integration might look like the following. [diagram]

The organization runs 8 peers that bootstrap from long-lived peers and introduce themselves to the rest of the Libri network. They may also have an internal service that uses the Author client library to proxy Put and Get requests to Libri via their 8 Librarian peers. This service may also send publication notifications on to an internal message buse (like Kafka) or filter them down to only those involving that organization (via their set of public keys). If they see that someone just shared a document with one of their public keys, they could then use Author client library download, decrypt, decompress, and concatenate the relevant Pages before storing in their own internal data system (which presumably uses its own encryption at rest and in transit).

Smaller organizations and almost all consumers will not want to run their own peers. We expect an ecosystem of 3rd-party companies to build consumer-facing apps and APIs that will proxy access to the data in Libri much like companies like Coinbase proxy a consmer's access to the underlying Bitcoin network.


\section{Experiments}
\label{sec:exps}
% - cluster performance over size & load
% 	- {8, 16, 32, 64}-node cluster @ {64K, 256K, 1024K} UPD
% - specific scenarios
% 	- peer drop-out triggering re-replication             
% 	- malicious Getters & Putters

Decentralized storage systems like IPFS, Sia, and Storj have existed for at least a few years, we have found very few empirical examinations of their performance. We believe that part of the evaluation for systems like these should include how efficiently they are able to handle the routine operations required of them. Below we describe some preliminary experiments on ephemeral Libri clusters.

\subsection{Performance across cluster size and load}

In this first set of initial experiments, we examined how a Libri cluster performs as the load on it and cluster size increase. In theory \ttt{Get} and \texttt{Put} requests in Kademlia-based architecture should scale as $O(log(n))$ for a cluster with $n$ peers, but we wanted to examine the extent to which these latencies scale in practice as $n$ increases. We also wanted to understand how well each of these clusters handles increasing client request load. We used the \texttt{Get} and \texttt{Put} endpoint latencies as our primary metrics of interest during these experiments, though other metrics like data throughput and cluster queries per second (QPS) are also relevant. Centralized key-value stores like DynamoDB, BigTable, Cassandra, and Riak boast request latencies in the single-digit millisecond ranges at potentially thousands of queries per second. While decentralized storage systems will never be able to match that performance, we believe that it's valuable to strive toward roughly the same orders of magnitude if decentralized storage is going to be used in a production setting. 

\subsubsection{Methods}
We simulated hypothetical user load with random documents in order to examine each cluster's performance. While the real distribution in healthcare document sizes is hard to know and will probably be very wide, we assume here that we are working with PDFs (unfortunately, still one of the most common forms of data exchange between healthcare organizations) on the order of a few hundred KBs. Documents were generated by first sampling a byte length from a gamma distribution with shape 1.5 and scale 170 (implying a mean of 256 KB and 95\% interval of [18, 794] KB and then randomly generating document content of the sampled length.

In an attempt to root cluster load in our expected real-world use case of uploading and sharing health-related documents, we define our load by uploads per day (UPD). For this experiment, we assume that each "upload" involves four distinct \ttt{Put}s: the \Entry{}, the self-shared \Envelope{}, and two additional \Envelope{}s representing sharing the document with two other parties. We also simulate each of the share parties downloading both the \Envelope{} with their public key and original \Entry{}, resulting in four total \texttt{Get} requests for each upload. Each "upload" thus involves, directly and indirectly, 8 total requests to the Libri network. 

We ran 12 trials, each with an ephemeral Libri cluster deployed using Kubernetes on infrastructure provistioned in Google Cloud Platform via Terraform. Each Librarian received 1 CPU, 6 GB RAM, and 10 GB network-attached SSD storage. Request latencies were captured via Prometheus monitoring and visualized in Grafana dashboards. We tested the cluster sizes (i.e., number of Librarian peers) 8, 16, 32, \& 64 and 64K, 256K, and 1024K UPD. Each trial ran for an hour, since the primary goal of this experiment was to measure short-term request latencies rather than long term cluster behavior. 

\begin{table}[t]
\centering
\begin{tabular}{rrr}
	\toprule
	user load (UPD) & \Put{} \ttt{+} \Get{} rate (QPS) & \Put{} throughput (Mbps) \\ \midrule
	64k & 5.8 & 4.3 \\
	256k & 23.7 & 18.9 \\
	1024k & 92.2 & 73.2 \\ \bottomrule
\end{tabular}
\caption{Query rate and throughput for each uploads per day (UPD) load. Results are shown for the 64-peer cluster, but those for other cluster sizes were similar.}
\end{table}

Request latencies were measures via the 50- and 95-percentile values of the latecy distributions (a.k.a., p50 and p95, respectively) for the \ttt{Put} and \texttt{Get} endpoints. These quantiles are estimated by Prometheus via its historgram counts of the request latencies (ref: https://prometheus.io/docs/practices/histograms/).  

\subsubsection{Results and Discussion}

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{figs/latencies.pdf}
	\caption{\Get{} and \Put{} request median (p50) and \nth{95} percentile (p95) latencies across different cluster sizes and user loads. Each colored line denotes a different user load, in uploads per day.}
\end{figure}

Figure X shows the \ttt{Put} and \texttt{Get} latency estimates aggregated over all Librarians as the cluster size increases from 8 to 64 peers. In each of the four charts, the 1024K UPD load almost always has the best latency performance. While this result may initially seem counterintuitive, it is almost certainly an artifact of the way Prometheus calculates quantiles from histograms, which tend to skew a little more conservative when the counts in each histogram bin are lower. 

We first note the order of magnitude of the latencies. Median \ttt{Put} latencies are in the tens of milliseconds, and p95s are generally between 100-250ms. Median \texttt{Get} latencies are in the single-digit milliseconds, and p95s are generally in the mid-to-high tens of milliseconds. The 8-Librarian cluster receiving 1024 UPD seems a little overwhelmed, but increasing the cluster size to 16 and then 32 peers seems to spread the load out effectively. In all but the \texttt{Put} p50, increasing the cluster size has little effect on the latencies, showing that the increased communications required with a larger cluster have little effect on the overall latency. The increase in \texttt{Put} p50 as the cluster size grows makes sense because each \texttt{Put} has to find the $k$ Librarians closest to a document key to send \texttt{Store} requests to. As the cluster size $n$ increases, this search time will grow proportional to $\log(n)$, and indeed the shape of the 64K and 256K UPD tends does seem roughly logarithmic. 

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{figs/qps-throughput.pdf}
	\caption{Query rate (in queries per second, or QPS) and \Put{} throughput (Megabits per second, or Mbps) across entire cluster and for an average peer. Each colored line denotes a different user load, in uploads per day. Standard errorbars on peer mean charts are omitted because they are too small to meaningfully resolve.}
\end{figure}

Figure X shows the queries per second (QPS) of the cluster overall and the average per peer as the UPD load and cluster size vary. In the cluster QPS plots, we see that the \ttt{Get + Put} QPS remains quite stable across the cluster size, with the exception that 1024 UPD on the 8-peer cluster is overloaded and thus has a slightly lower QPS than other 1024 UPS trials at larger cluster sizes. But the total cluster QPS shows a fairly consistent (again with the exception of 1024 UPD on the 8-peer cluster) increasing linear trend of roughly 4 QPS/peer, meaning that each additional peer adds roughly 4 QPS to the total cluster QPS.

The total average peer QPS predictable drops as the cluster size increases for each UPD load, but since a larger cluster requires more inter-node communication for each \ttt{Put} and \texttt{Get}, the drop is not linear but more of an exponential decay. Since we know that 1024 UPD on an 8-peer cluster had anomalously poor latency results while the 16-peer clusters served that same load with good performance, we posit a roughly 50 QPS limit per peer (between the 72 QPS on the 8-peer cluster and 41 QPS on the 16-peer cluster) in order to maintain good latency performance.

Figure X shows the cluster and average peer storage (\ttt{Put} endpoint) throughput for each UPD load and cluster size. The cluster-wide throughputs are quite consistent across cluster size with the slightly lower throughput at 1024 UPD on the 8-peer cluster. At 1024 UPD and cluster sizes 16 and larger, the cluster throughput is roughly 72 Mbps. As above with QPS, we might similarly posit a roughly 5 Mbps limit per peer in order to maintain good cluster performance.

The latency, request rate, and throughput results from this experiment show how a modest Libri cluster can achieve quite good performance: on average, \ttt{Get}s in the single-digit milliseconds and \texttt{Puts} in the tens of milliseconds when serving 80 \texttt{Get + Put} QPS with a 72 Mbps throughput. These experiments also showed that while increasing the cluster size increases the number of requests within the cluster, its effects on \texttt{Put} and \texttt{Get} latencies are small, and its effects on overall QPS and throughput are undetectable. These results give us initial confidence that a Libri cluster will be able to scale well with the first wave of users and load associated in the first few years of its existance.


\section{Future Work}
\label{sec:future}
% - community: fully open source
% 	- development and initial hosting led by Elixir Health
% 		- will offer optional paid services
% 			- mobile & web apps
% 			- identity management
% 			- proxy APIs to libri
% 	- welcome all members of healthcare ecosystem
% 		- doctors & healthcare systems
% 		- public and private insurance organizations
% 		- life sciences & device companies
% 		- public health & medical research organizations
% 		- consumer health organizations (gym, step & diet trackers)
% - future work
% 	- data formats
% 	- integrations with healthcare organizations
% 	- more exps
% 		- heterogenous clusters across diff orgs & AZs
% 		- malicious actors
% 	- wild staging & prod cluster across orgs

Libri is and will always be a fully open source network. Openness engenders trust, and trust is critital when managing something as important and sensitive as long-term health records. We hope to build a vibrant open source community, where prioritization, important features, and bugfixes are contributed as many participants in the network as possible. 

Core development will be led by Elixir Health, which will offer optional paid services on top of Libri, including web and mobile app, identity and access management, and a proxy API to Libri for other \nth{3} party organizations and apps not yet interested in running their own nodes.

Libri and Elixir Health welcome all members of our healthcare ecoystem: doctors and healthcare systems, public and private insurance organizations, life sciences and device companies, public health and medical research groups, and consumer health organizations like gyms, fitness, and diet trackers. Each organization has a role to play in and much to benefit from a shared, accessible repository of all health data.

The next large technical hurdle is ramping up integrations with healthcare organizations. Initially, some records will be unstructured (or semi-structured) (PDFs, images) and some will be structured (HL7, FHIR, CCR). Easily storing, accessing, and sharing these documents---regardless of the form---dramatically improves on the piecemeal system we have now. Once this unstandardized, heterogenous data flows easily, our collective incentives for better standards, compatibility, and documentation increase. We expect to spend a great deal of collaborative energy on this front. The schema and data dictionary specification given in \ttt{EntryMetadata} is intended to start us off on that direction. 

\begin{appendices}

\section{Document structure}
\label{app:docs}

Below we describe the structure of each of the three Libri document types: \Page{}, \Entry{}, and \Envelope{}. We use Protobuf types and add a length to \ttt{bytes} arrays when it is fixed. Each document key (e.g., \ttt{entry\_key}) is the SHA-256 hash of the document serialized to binary.

\begin{table}[htbp]
	\caption*{\Page{} structure}
	\begin{small}
	\begin{tabular}{lll}
		\toprule
		name & type  & description \\ \midrule
		\ttt{author\_public\_key} & \ttt{bytes[32]} & public key of author/sender \\
		\ttt{index} & \ttt{uint32} & index of this \Page{} in the \Entry{} \\
		\ttt{ciphertext} & \ttt{bytes} & encrypted contents of \Page{}, up to 2 MB \\
		\ttt{ciphertext\_mac} & \ttt{bytes[32]} & ciphertext HMAC \\
		\bottomrule
	\end{tabular}
	\end{small}
\end{table}

\begin{table}[htbp]
	\caption*{\Entry{} structure}
	\begin{small}
	\begin{tabular}{lll}
		\toprule
		name & type  & description \\ \midrule
		\ttt{author\_public\_key} & \ttt{bytes[32]} & public key of author/sender \\
		\ttt{page} & \ttt{Page} & for single-\Page{} content \\
		\ttt{page\_keys} & \ttt{repeated bytes[32]} & for multi-\Page{} content \\
		\ttt{created\_time} & \ttt{uint32} & client epoch time when \Entry{} created \\
		\ttt{metadata\_ciphertext} & \ttt{bytes} & ciphertext of serialized \ttt{EntryMetadata} \\
		\ttt{metadata\_ciphertext\_mac} & \ttt{bytes} & \ttt{EntryMetadata} ciphertext MAC \\
		\bottomrule
	\end{tabular}
	\end{small}
\end{table}


\begin{table}[htbp]
	\caption*{\Envelope{} structure}
	\begin{small}
	\begin{tabular}{lll}
		\toprule
		name & type  & description \\ \midrule
		\ttt{entry\_key} & \ttt{bytes[32]} & document key of \Entry{} whose EEK is encrypted \\
		\ttt{author\_public\_key} & \ttt{bytes[32]} & public key of author/sender \\
		\ttt{reader\_public\_key} & \ttt{bytes[32]} & public key of reader/receiver \\
		\ttt{eek\_ciphertext}  & \ttt{bytes[124]} & encrypted EEK (including 16-byte encryption info) \\
		\ttt{eek\_ciphertext\_mac} & \ttt{bytes[32]} & EEK ciphertext HMAC \\
		\bottomrule
	\end{tabular}
	\end{small}
\end{table}

\section{Entry metadata}
\label{app:entry-meta}

TODO

\end{appendices}

\printbibliography

\end{document}
